# nlp5
In this project, we also mention cosine similarities, as well as pre-processing done in English, such as: encoding where our inputs are in the form of sentences, but our outputs are: lists of words. Next pre-processing in English, including cleaning, encoding... Stop words: list of English stop words and output document string: list of words deletes redundant data and is marking and moving keywords. In the meantime, remove the words outside the vocabulary and the function: Check if a document has a vector representation. Input: :doc is a list of words and output: Boolean value: True: doc is not null and has a vector representation. False: doc is empty or a vector representation. does not have. We check if null doc Check if there is at least one word from document in the vec2word dictionary. In addition to the performed operations, we also check the similarity calculation, which checks the similarity between the texts and gives a value between 1 and 0, but if it gives a value of -1, then we check whether the number of documents matches or not. And change the setting according to the language of the text. After calling the data, we will normalize them, we will process the vector, we will calculate the data and the similarities and perform the regularization. In the next step, we read the data, we calculated the similarities, we start to store it inside the main, we read the data and process it.
For the IR file system in the first part we call the libraries and in the data preprocessing we read the data making sure we only get the files we really want and make sure everything is small And remove the split on white space and non-alphabetic characters and remove any words that are now empty and add stem words and to the contents of the document also make sure we only get the files we really want. In the next step, make sure that we only add the files that we really want to receive to the document contents. According to the location of the 'data' directory, it will be indexed in the documentation. Note: we save root documents for speed (eg write "/stemmed" on files with new dir). dict map filename to list of "words" (tokens) Alphabetize the document to be sure which is appropriate Document index when referring to them Get the glossary Compute tfidf 2l norms of each document for use in cosine similarity The frequency term d,tft term t in document d as the number of times t occurs in d gives is defined. TODO: Return the idf-tf weight for the given word (string) and document index. This IDF-TF function receives an unstemmed word in a document. he does. Roots the word and then calls tfidf_get. You shouldn't *need* to change this interface, but it is necessary for submission. We make a list of documents. TODO: Create a reverse index. Of course, this may not be a linked list like a proper implementation. Some useful sample variables: * = docs.self List of documents * = titles.self List of titles Given a word, returns a (sorted) list of document indices in which the word occurs. TODO: List A Given a word , this is the *source* of the word, and then calls posting_get on the root word to get its list of posts. You *shouldn't* change this function. It's required for posting. Given a query In the form of a list of *root* words, this list returns documents containing *all* of those words (i.e., an ANDquery). If the query returns no documents, return an empty list. TODO: execute Boolean retrieval. You want to use your reverse index that you created in index(). Currently this just returns all possible documents! Sorted doesn't really matter. Return posts for a word. Given a query (list of words), return the ranked list documents (by ID) and score the query. Calculate the scores and add to the priority queue. Return the top 10 scores. By giving a query string, process it and return a list of lowercase, alphabetic, and root words in the string Make sure everything is lowercase Split on spaces Remove non-alphabetic characters Root words Given a string, list Process and then return the matching documents found by (retrieve_boolean ). Given a string, process and then rank the list of top matching documents in order
We also test everything in the test file.
